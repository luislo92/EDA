{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection Using AutoEncoders for Credit Card Data\n",
    "\n",
    "\n",
    "* Motivation:\n",
    "\n",
    "Detecting outliers has become one of the most significant skillset to have for data scientist. Outliers usually pinpoints data scientist towards abnormal behavior and this abnormal behavior can then be analyzed to understand what's causing it. In the following assignment I will use the library PyOd and its modules to run autoencoder models that can detect outliers.\n",
    "\n",
    "\n",
    "* Resources:\n",
    "        \n",
    "        *Modified Dataset on Credit Card Transactions.\n",
    "        *Outlier Detection Models.\n",
    "        *Visualization tools to determine thresholds.\n",
    "        \n",
    "* Methods:\n",
    "        \n",
    "        *PyOd Library to run an autoencoder algorithm.\n",
    "        *CrossValidate Results using Average and Maximization.\n",
    "        \n",
    "        \n",
    "* Objective:\n",
    "\n",
    "By using the already modified dataset on Credit Card Transaction Data identify which observations can be described as outliers and provide a brief description as to why the algorithm picked those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Loading the Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pyod.models.combination import aom, moa, average, maximization\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Setting up the Data\n",
    "os.chdir(\"/Users/luislosada/Columbia Drive/Anomaly Detection/Module 7\")\n",
    "flag_data = pd.read_csv(\"flag_aggregated_dataset.csv\",index_col=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flag_Pos_Neg_Mean</th>\n",
       "      <th>Flag_Ratio_of_Unique_Transactions_Mean</th>\n",
       "      <th>flag_amount_bin</th>\n",
       "      <th>flag_week_amount</th>\n",
       "      <th>flag_month_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39886.000000</td>\n",
       "      <td>39886.000000</td>\n",
       "      <td>39886.000000</td>\n",
       "      <td>39886.000000</td>\n",
       "      <td>39886.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.111329</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.151595</td>\n",
       "      <td>0.067518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.029430</td>\n",
       "      <td>0.299258</td>\n",
       "      <td>0.021926</td>\n",
       "      <td>0.293093</td>\n",
       "      <td>0.212029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Flag_Pos_Neg_Mean  Flag_Ratio_of_Unique_Transactions_Mean  \\\n",
       "count       39886.000000                            39886.000000   \n",
       "mean            0.001174                                0.111329   \n",
       "std             0.029430                                0.299258   \n",
       "min             0.000000                                0.000000   \n",
       "25%             0.000000                                0.000000   \n",
       "50%             0.000000                                0.000000   \n",
       "75%             0.000000                                0.000000   \n",
       "max             1.000000                                1.000000   \n",
       "\n",
       "       flag_amount_bin  flag_week_amount  flag_month_amount  \n",
       "count     39886.000000      39886.000000       39886.000000  \n",
       "mean          0.000994          0.151595           0.067518  \n",
       "std           0.021926          0.293093           0.212029  \n",
       "min           0.000000          0.000000           0.000000  \n",
       "25%           0.000000          0.000000           0.000000  \n",
       "50%           0.000000          0.000000           0.000000  \n",
       "75%           0.000000          0.160000           0.000000  \n",
       "max           1.000000          1.000000           1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag_data.iloc[:,3:].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Splitting the Data to test out the results of the autoencoder \n",
    "X_train, X_test = train_test_split(flag_data.iloc[:,3:], test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders\n",
    "\n",
    "Autoencoders algorithms are unsupervised learning techniques that use neural networks to compress and encode data to later decode it and try to reconstruct it. The algorithm has 4 parts to it. The encoder where the model learns how to an encoded representation. A bottleneck which is the layer that contains the encoded representation of the input data. The decoder where the model learns how to reconstruct the data. The last part is the reconstruction loss which is the method that measures how well the encoder is performing. \n",
    "This last part is very useful for identifying outliers. If we train the data on a set of values then when get a loss value for the set. If we then test our model on new data and the loss value is above an established threshold that means that the data was not able to be reconstructed properly because and thus must be an outlier.\n",
    "For this assignment we will use the Autoencoder algorithm of PyOd to determine which values are outliers. I want to limit over and under fitting the data so I'll try different combination of hidden layers and input and output layers in order to validate my results.\n",
    "\n",
    "More on autoencoders [here]('https://towardsdatascience.com/auto-encoder-what-is-it-and-what-is-it-used-for-part-1-3e5c6f017726')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Python/3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Python/3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 25)                150       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 52        \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 25)                75        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 130       \n",
      "=================================================================\n",
      "Total params: 473\n",
      "Trainable params: 473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Library/Python/3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 28717 samples, validate on 3191 samples\n",
      "Epoch 1/100\n",
      "28717/28717 [==============================] - 2s 71us/step - loss: 9.4276 - val_loss: 6.9519\n",
      "Epoch 2/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 3.9919 - val_loss: 4.1593\n",
      "Epoch 3/100\n",
      "28717/28717 [==============================] - 1s 43us/step - loss: 2.5811 - val_loss: 3.0629\n",
      "Epoch 4/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 1.8849 - val_loss: 2.3155\n",
      "Epoch 5/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 1.4997 - val_loss: 1.8298\n",
      "Epoch 6/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 1.2669 - val_loss: 1.4776\n",
      "Epoch 7/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 1.1317 - val_loss: 1.2655\n",
      "Epoch 8/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 1.0590 - val_loss: 1.1519\n",
      "Epoch 9/100\n",
      "28717/28717 [==============================] - 1s 48us/step - loss: 1.0231 - val_loss: 1.0990\n",
      "Epoch 10/100\n",
      "28717/28717 [==============================] - 1s 47us/step - loss: 1.0071 - val_loss: 1.0799\n",
      "Epoch 11/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 1.0002 - val_loss: 1.0732\n",
      "Epoch 12/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9970 - val_loss: 1.0707\n",
      "Epoch 13/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 0.9953 - val_loss: 1.0695\n",
      "Epoch 14/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 0.9943 - val_loss: 1.0687\n",
      "Epoch 15/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9937 - val_loss: 1.0682\n",
      "Epoch 16/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 0.9933 - val_loss: 1.0679\n",
      "Epoch 17/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 0.9930 - val_loss: 1.0677\n",
      "Epoch 18/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 0.9929 - val_loss: 1.0676\n",
      "Epoch 19/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 0.9928 - val_loss: 1.0675\n",
      "Epoch 20/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 0.9927 - val_loss: 1.0675\n",
      "Epoch 21/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 0.9926 - val_loss: 1.0674\n",
      "Epoch 22/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 0.9926 - val_loss: 1.0674\n",
      "Epoch 23/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 0.9926 - val_loss: 1.0674\n",
      "Epoch 24/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9926 - val_loss: 1.0674\n",
      "Epoch 25/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9926 - val_loss: 1.0674\n",
      "Epoch 26/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9926 - val_loss: 1.0674\n",
      "Epoch 27/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 28/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 29/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 30/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 31/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 32/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 33/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 34/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 35/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 36/100\n",
      "28717/28717 [==============================] - 1s 47us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 37/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 38/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 39/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 40/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 41/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 42/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 43/100\n",
      "28717/28717 [==============================] - 1s 44us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 44/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 45/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 46/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 47/100\n",
      "28717/28717 [==============================] - 1s 47us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 48/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 49/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 51/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 52/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 53/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 54/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 55/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 56/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 57/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 58/100\n",
      "28717/28717 [==============================] - 1s 47us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 59/100\n",
      "28717/28717 [==============================] - 1s 47us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 60/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 61/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 62/100\n",
      "28717/28717 [==============================] - 1s 47us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 63/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 64/100\n",
      "28717/28717 [==============================] - 1s 47us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 65/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 66/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 67/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 68/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 69/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 70/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 71/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 72/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 73/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 74/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 75/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 76/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 77/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 78/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 79/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 80/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 81/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 82/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 83/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 84/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 85/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 86/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 87/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 88/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 89/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 90/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 91/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 92/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 93/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 94/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 95/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 96/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 97/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 98/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 99/100\n",
      "28717/28717 [==============================] - 1s 45us/step - loss: 0.9925 - val_loss: 1.0673\n",
      "Epoch 100/100\n",
      "28717/28717 [==============================] - 1s 46us/step - loss: 0.9925 - val_loss: 1.0673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoEncoder(batch_size=32, contamination=0.1, dropout_rate=0.2, epochs=100,\n",
       "      hidden_activation='relu', hidden_neurons=[25, 2, 2, 25],\n",
       "      l2_regularizer=0.1,\n",
       "      loss=<function mean_squared_error at 0x13b8c7400>, optimizer='adam',\n",
       "      output_activation='sigmoid', preprocessing=True, random_state=None,\n",
       "      validation_size=0.1, verbose=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = AutoEncoder(hidden_neurons =[25, 2, 2, 25])\n",
    "clf1.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7478\n",
       "1     500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_scores = clf1.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf1.predict(X_test)  # outlier labels (0 or 1)\n",
    "\n",
    "y_test_scores = clf1.decision_function(X_test)  # outlier scores\n",
    "\n",
    "y_test_pred = pd.Series(y_test_pred)\n",
    "y_test_scores = pd.Series(y_test_scores)\n",
    "\n",
    "y_test_pred.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7978.000000\n",
       "mean        1.374447\n",
       "std         1.905485\n",
       "min         0.098963\n",
       "25%         0.716627\n",
       "50%         0.716627\n",
       "75%         1.291131\n",
       "max        44.236115\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_test_scores,bins='auto')  \n",
    "plt.title(\"Histogram for Model Clf1 Anomaly Scores\")\n",
    "plt.xlim(-1, 2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most values fall between 0 and 2 and the rest of the values seem to be far off and widely spread. Lets create a threshold so that anything above 2 is flagged as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6770\n",
       "0    1208\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = X_test.copy()\n",
    "df_test['score'] = y_test_scores\n",
    "df_test['cluster'] = np.where(df_test['score']<2, 0, 1)\n",
    "df_test['cluster'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flag_Pos_Neg_Mean</th>\n",
       "      <th>Flag_Ratio_of_Unique_Transactions_Mean</th>\n",
       "      <th>flag_amount_bin</th>\n",
       "      <th>flag_week_amount</th>\n",
       "      <th>flag_month_amount</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.150807</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.169179</td>\n",
       "      <td>0.067354</td>\n",
       "      <td>0.751461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.099864</td>\n",
       "      <td>0.00099</td>\n",
       "      <td>0.149697</td>\n",
       "      <td>0.064564</td>\n",
       "      <td>3.729256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Flag_Pos_Neg_Mean  Flag_Ratio_of_Unique_Transactions_Mean  \\\n",
       "cluster                                                              \n",
       "0                 0.000248                                0.150807   \n",
       "1                 0.001958                                0.099864   \n",
       "\n",
       "         flag_amount_bin  flag_week_amount  flag_month_amount     score  \n",
       "cluster                                                                  \n",
       "0                0.00000          0.169179           0.067354  0.751461  \n",
       "1                0.00099          0.149697           0.064564  3.729256  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby('cluster').mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the huge spread between the cluster classified as normal and the cluster classified as outlier. The average score is more than 4 points off. It seems then that the cutoff point at 2 was the right choice.\n",
    "\n",
    "Lets now test this theory by using more hidden layers and encoding-decoding the data two (clf2) and three (clf3) times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 25)                150       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 25)                275       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 130       \n",
      "=================================================================\n",
      "Total params: 927\n",
      "Trainable params: 927\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 28717 samples, validate on 3191 samples\n",
      "Epoch 1/100\n",
      "28717/28717 [==============================] - 2s 80us/step - loss: 6.0940 - val_loss: 3.2143\n",
      "Epoch 2/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 2.3786 - val_loss: 2.1192\n",
      "Epoch 3/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 1.7031 - val_loss: 1.6808\n",
      "Epoch 4/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 1.3663 - val_loss: 1.4408\n",
      "Epoch 5/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 1.1848 - val_loss: 1.3177\n",
      "Epoch 6/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 1.0863 - val_loss: 1.2535\n",
      "Epoch 7/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 1.0388 - val_loss: 1.2209\n",
      "Epoch 8/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 1.0141 - val_loss: 1.2028\n",
      "Epoch 9/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 1.0002 - val_loss: 1.1923\n",
      "Epoch 10/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9921 - val_loss: 1.1862\n",
      "Epoch 11/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9873 - val_loss: 1.1827\n",
      "Epoch 12/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9845 - val_loss: 1.1806\n",
      "Epoch 13/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9829 - val_loss: 1.1794\n",
      "Epoch 14/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9819 - val_loss: 1.1787\n",
      "Epoch 15/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9813 - val_loss: 1.1782\n",
      "Epoch 16/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9809 - val_loss: 1.1780\n",
      "Epoch 17/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9807 - val_loss: 1.1778\n",
      "Epoch 18/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9806 - val_loss: 1.1777\n",
      "Epoch 19/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9805 - val_loss: 1.1777\n",
      "Epoch 20/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9804 - val_loss: 1.1776\n",
      "Epoch 21/100\n",
      "28717/28717 [==============================] - 2s 52us/step - loss: 0.9804 - val_loss: 1.1776\n",
      "Epoch 22/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9804 - val_loss: 1.1776\n",
      "Epoch 23/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 24/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 25/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 26/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 27/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 28/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 29/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 30/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 31/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 32/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 33/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 34/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 35/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 36/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 37/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 38/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 39/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 40/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 41/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 42/100\n",
      "28717/28717 [==============================] - 2s 53us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 43/100\n",
      "28717/28717 [==============================] - 2s 55us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 44/100\n",
      "28717/28717 [==============================] - 2s 54us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 45/100\n",
      "28717/28717 [==============================] - 2s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 46/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 47/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 48/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 49/100\n",
      "28717/28717 [==============================] - 2s 53us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 50/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 51/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 52/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 53/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 54/100\n",
      "28717/28717 [==============================] - 2s 54us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28717/28717 [==============================] - 2s 53us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 56/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 57/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 58/100\n",
      "28717/28717 [==============================] - 2s 53us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 59/100\n",
      "28717/28717 [==============================] - 2s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 60/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 61/100\n",
      "28717/28717 [==============================] - 2s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 62/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 63/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 64/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 65/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 66/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 67/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 68/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 69/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 70/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 71/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 72/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 73/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 74/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 75/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 76/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 77/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 78/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 79/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 80/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 81/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 82/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 83/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 84/100\n",
      "28717/28717 [==============================] - 2s 53us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 85/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 86/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 87/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 88/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 89/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 90/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 91/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 92/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 93/100\n",
      "28717/28717 [==============================] - 1s 51us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 94/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 95/100\n",
      "28717/28717 [==============================] - 2s 53us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 96/100\n",
      "28717/28717 [==============================] - 2s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 97/100\n",
      "28717/28717 [==============================] - 2s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 98/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 99/100\n",
      "28717/28717 [==============================] - 2s 53us/step - loss: 0.9803 - val_loss: 1.1775\n",
      "Epoch 100/100\n",
      "28717/28717 [==============================] - 1s 52us/step - loss: 0.9803 - val_loss: 1.1775\n"
     ]
    }
   ],
   "source": [
    "clf2 = AutoEncoder(hidden_neurons = [25, 10, 2, 10, 25])\n",
    "clf2.fit(X_train)\n",
    "y_test_scores_2 = clf2.decision_function(X_test)  # outlier scores\n",
    "y_test_scores_2 = pd.Series(y_test_scores_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 25)                150       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 15)                390       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 25)                400       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 5)                 130       \n",
      "=================================================================\n",
      "Total params: 1,507\n",
      "Trainable params: 1,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 28717 samples, validate on 3191 samples\n",
      "Epoch 1/100\n",
      "28717/28717 [==============================] - 3s 93us/step - loss: 8.8650 - val_loss: 5.7039\n",
      "Epoch 2/100\n",
      "28717/28717 [==============================] - 2s 60us/step - loss: 3.8180 - val_loss: 3.4541\n",
      "Epoch 3/100\n",
      "28717/28717 [==============================] - 2s 60us/step - loss: 2.4566 - val_loss: 2.2248\n",
      "Epoch 4/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 1.7296 - val_loss: 1.6020\n",
      "Epoch 5/100\n",
      "28717/28717 [==============================] - 2s 60us/step - loss: 1.3429 - val_loss: 1.3162\n",
      "Epoch 6/100\n",
      "28717/28717 [==============================] - 2s 60us/step - loss: 1.1504 - val_loss: 1.1966\n",
      "Epoch 7/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 1.0647 - val_loss: 1.1495\n",
      "Epoch 8/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 1.0272 - val_loss: 1.1275\n",
      "Epoch 9/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 1.0096 - val_loss: 1.1161\n",
      "Epoch 10/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 1.0007 - val_loss: 1.1098\n",
      "Epoch 11/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9959 - val_loss: 1.1062\n",
      "Epoch 12/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9931 - val_loss: 1.1041\n",
      "Epoch 13/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9914 - val_loss: 1.1029\n",
      "Epoch 14/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9904 - val_loss: 1.1022\n",
      "Epoch 15/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9898 - val_loss: 1.1017\n",
      "Epoch 16/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9895 - val_loss: 1.1014\n",
      "Epoch 17/100\n",
      "28717/28717 [==============================] - 2s 63us/step - loss: 0.9892 - val_loss: 1.1013\n",
      "Epoch 18/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9891 - val_loss: 1.1011\n",
      "Epoch 19/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9890 - val_loss: 1.1011\n",
      "Epoch 20/100\n",
      "28717/28717 [==============================] - 2s 63us/step - loss: 0.9889 - val_loss: 1.1010\n",
      "Epoch 21/100\n",
      "28717/28717 [==============================] - 2s 64us/step - loss: 0.9889 - val_loss: 1.1010\n",
      "Epoch 22/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9889 - val_loss: 1.1010\n",
      "Epoch 23/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9888 - val_loss: 1.1010\n",
      "Epoch 24/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9888 - val_loss: 1.1010\n",
      "Epoch 25/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 26/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 27/100\n",
      "28717/28717 [==============================] - 2s 60us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 28/100\n",
      "28717/28717 [==============================] - 2s 60us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 29/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 30/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 31/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 32/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 33/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 34/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 35/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 36/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 37/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 38/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 39/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 40/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 41/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 42/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 43/100\n",
      "28717/28717 [==============================] - 2s 61us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 44/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 45/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 46/100\n",
      "28717/28717 [==============================] - 2s 62us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 47/100\n",
      "28717/28717 [==============================] - 2s 60us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 48/100\n",
      "28717/28717 [==============================] - 2s 58us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 49/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28717/28717 [==============================] - 2s 58us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 51/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 52/100\n",
      "28717/28717 [==============================] - 2s 58us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 53/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 54/100\n",
      "28717/28717 [==============================] - 2s 58us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 55/100\n",
      "28717/28717 [==============================] - 2s 58us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 56/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 57/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 58/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 59/100\n",
      "28717/28717 [==============================] - 2s 58us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 60/100\n",
      "28717/28717 [==============================] - 2s 58us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 61/100\n",
      "28717/28717 [==============================] - 2s 58us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 62/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 63/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 64/100\n",
      "28717/28717 [==============================] - 2s 58us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 65/100\n",
      "28717/28717 [==============================] - 2s 58us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 66/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 67/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 68/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 69/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 70/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 71/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 72/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 73/100\n",
      "28717/28717 [==============================] - 2s 63us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 74/100\n",
      "28717/28717 [==============================] - 2s 59us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 75/100\n",
      "28717/28717 [==============================] - 2s 55us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 76/100\n",
      "28717/28717 [==============================] - 2s 55us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 77/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 78/100\n",
      "28717/28717 [==============================] - 2s 63us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 79/100\n",
      "28717/28717 [==============================] - 2s 63us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 80/100\n",
      "28717/28717 [==============================] - 2s 60us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 81/100\n",
      "28717/28717 [==============================] - 2s 59us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 82/100\n",
      "28717/28717 [==============================] - 2s 59us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 83/100\n",
      "28717/28717 [==============================] - 2s 59us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 84/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 85/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 86/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 87/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 88/100\n",
      "28717/28717 [==============================] - 2s 57us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 89/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 90/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 91/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 92/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 93/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 94/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 95/100\n",
      "28717/28717 [==============================] - 2s 56us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 96/100\n",
      "28717/28717 [==============================] - 2s 55us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 97/100\n",
      "28717/28717 [==============================] - 2s 55us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 98/100\n",
      "28717/28717 [==============================] - 2s 55us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 99/100\n",
      "28717/28717 [==============================] - 2s 55us/step - loss: 0.9888 - val_loss: 1.1009\n",
      "Epoch 100/100\n",
      "28717/28717 [==============================] - 2s 55us/step - loss: 0.9888 - val_loss: 1.1009\n"
     ]
    }
   ],
   "source": [
    "clf3 = AutoEncoder(hidden_neurons =[25, 15, 10, 2, 10, 15, 25])\n",
    "clf3.fit(X_train)\n",
    "y_test_scores_3 = clf3.decision_function(X_test)  # outlier scores\n",
    "y_test_scores_3 = pd.Series(y_test_scores_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Creating a dataset to compare results.\n",
    "train_scores = pd.DataFrame({'clf1': clf1.decision_scores_,\n",
    "                             'clf2': clf2.decision_scores_,\n",
    "                             'clf3': clf3.decision_scores_\n",
    "                            })\n",
    "\n",
    "test_scores  = pd.DataFrame({'clf1': clf1.decision_function(X_test),\n",
    "                             'clf2': clf2.decision_function(X_test),\n",
    "                             'clf3': clf3.decision_function(X_test) \n",
    "                            })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although my original data set was already normalized the scores obtained are not, and as seen in the description of the data above, there are huge outliers in the data. Lets then transform the data using a Z-score transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_scores_norm =  StandardScaler().fit_transform(train_scores)\n",
    "test_scores_norm =  StandardScaler().fit_transform(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Combination\n",
    "\n",
    "A great to validate our scores is to combine the scores of three models built. There are several techniques for this but in this case, I will use two methods:\n",
    "\n",
    "- Average: Which takes the average score of all three methods produced above and creates an aggregated score.\n",
    "- Max Score: Takes the max score of all three methods and outputs that score as the score for that observation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7.978000e+03\n",
       "mean     1.536333e-17\n",
       "std      1.000063e+00\n",
       "min     -6.694319e-01\n",
       "25%     -3.452413e-01\n",
       "50%     -3.452413e-01\n",
       "75%     -4.373359e-02\n",
       "max      2.249525e+01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Average using PyOd\n",
    "y_by_average = average(test_scores_norm)\n",
    "pd.Series(y_by_average).describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this average score, we take into account the results of all models built and produce a score that takes into account all three models, therefore reducing the chance of overfitting and underfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFVdJREFUeJzt3XuwpVV95vHvQzcXLwSQ7iA0LY0JFYPOREkXYHQmFBhp0bJJggmZRBqHSKiBKiwzUcwNByFBq4yXiZegULZO5BJ0AkEdh+tYTgLYIII0gzSKQ3caaOWiKDKCv/ljr7OyPZzT59rn0vl+qnad9a53ve+71n7P2c9+L3ufVBWSJAHsMt8dkCQtHIaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQfMqyTuT/LftzL8zyVE7aNsfTfJnO2C92x2TtJAZChpTkv+QZEOSx5NsTfKFJK+c635U1Yur6oaZrifJyUm+PGrdp1XVu2a6bmlnYijoGZK8FXg/8BfAfsALgA8Da+ezX9oxkiyd7z5o4TAU9FOS7AWcA5xeVZ+tqh9U1Y+r6h+q6o9am92TvD/JP7fH+5Ps3uYdlWRzkrcleagdZRyf5Lgk30jycJI/HrXZPZJcmuT7SW5N8ktD/bkvyata+Z1JLkvyydb2ziSrh9qeleTeNm9jkl9v9b8IfBR4eTvyebTVfyLJuUPLvznJptbHK5McMDSvkpyW5J4kjyb5UJJs56kcc0xJ/ijJZ0Y95x9M8oFx9sd4Y9q99eMlQ22XJ3kiyc+26dclua21+8ck/3bU8/r2JLcDP0iydLxttfZLkrw3yXeSfCvJGe05Wdrm75Xkwra/tyQ5N8mS7Tw/WqiqyoeP/gDWAE8BS7fT5hzgRuBngeXAPwLvavOOasv/ObAr8GZgG/BpYE/gxcATwMGt/TuBHwMntPb/GfgWsGubfx/wqqG2PwKOA5YAfwncONSvNwAHMHiz89vAD4D927yTgS+PGscngHNb+WjgO8BhwO7AfwW+NNS2gKuAvRkcOW0D1ozz/Iw7JmD/1q+9W9ulwEPAL4+zru2N6SLgvKG2pwP/o5Vf1tZ7RHuu1rXncveh5/U2YCXwrEls6zRgI3AgsA9wTXtOlrb5/x34G+A5DH4vbgb+YL5/n31M4zVgvjvgY2E9gN8FHpigzb3AcUPTxwL3tfJRDF70l7TpPduLxxFD7W8Bjm/ld456Yd8F2Ar8uzZ9Hz8dCtcMtT0UeGI7/bwNWNvKJ7P9ULgQeM/QvOe2F/ZVbbqAVw7Nvww4a5ztTjSmLwBvbuXXARunsH+Gx/Qq4N6hef8bOKmVP0IL6qH5dwO/OvS8/scpbOu64Rf5tu1iEGr7AU/SwqXN/x3g+vn+ffYx9YenjzTad4FlE5xnPgD49tD0t1tdX0dVPd3KT7SfDw7Nf4LBi+6I+0cKVfUTYPOo9Q17YKj8QwanaUZOYZw0dLrkUeAlwLLtjGPYT42pqh5n8Fys2M62h8cw2vbGtB74vVb+PeBT461kgjFdDzw7yRFJVgEvZfCOHeAg4A9HlmvLruSnn9f7h8oTbeuAUe2HywcxOAraOrTs3zA4YtAi4wUmjfZPDN71HQ9cPk6bf2bwQnBnm35Bq5uulSOFJLswOEUxpfUlOQj4GHAM8E9V9XSS24CR8/4TfR3wyJhG1vccYF9gy1T6MWR7Y/p74CPtesDrgLeNtYKJxtSmL2PwrvxB4Kqq+n5b/H4Gp5bO204f+3MyiedvaxvDM8bXtvUksKyqntrO9rQIeKSgn1JVjzG4HvChdoH42Ul2TfKaJO9pzS4G/rRd2FzW2s/kvvxfTvIb7R3/Wxi8wNw4xXU8h8GL3DaAJG9i8E53xIPAgUl2G2f5i4E3JXlpBhfN/wK4qarum2I/Row7pqr6EYPA/TRwc1X932mOibaO32Zw2u/TQ/UfA05rRxFJ8pwkr02y5zS3dRlwZpIVSfYG3j4yo6q2Av8TeG+Sn0myS5KfS/Kr4z47WrAMBT1DVb0XeCvwpwxeJO4HzmDwDhfgXGADcDtwB3Brq5uuKxi8sD0CvBH4jar68RT7vBF4L4MjnQeBf8PgHPuI6xgc2TyQ5DtjLH8N8GfAZxi8K/454MQpj+RfTDSm9a2P4546msSYqKqbGFwQPoDBtYqR+g0MLvL/devDJgbXVaa7rY8xeOG/Hfgq8HkGNxSMnCY8CdiNwcXoRxiE3v7jbU8LV6r8JzvSXEvyAuD/AM+vqu/Nd3+mKslrgI9W1UETNtai4pGCNMfaNYa3ApcslkBI8qwMPmuyNMkK4Gz+5aK2diIeKUhzqF3AfpDBnU5rqur+CRZZEJI8G/hfwIsY3D32OeDMxRJqmjxDQZLUefpIktQt6M8pLFu2rFatWjXf3ZCkReWWW275TlUtn86yCzoUVq1axYYNG+a7G5K0qCT59sStxubpI0lSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVK3oD/RrPGtOutzvXzf+a+dx55I2pl4pCBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdZMOhSRLknw1yVVt+uAkNyXZlOTSJLu1+t3b9KY2f9XQOt7R6u9OcuxsD0aSNDNTOVI4E7hraPrdwPuq6ueBR4BTWv0pwCOt/n2tHUkOBU4EXgysAT6cZMnMui9Jmk2TCoUkBwKvBT7epgMcDVzemqwHjm/ltW2aNv+Y1n4tcElVPVlV3wI2AYfPxiAkSbNjskcK7wfeBvykTe8LPFpVT7XpzcCKVl4B3A/Q5j/W2vf6MZbpkpyaZEOSDdu2bZvCUCRJMzVhKCR5HfBQVd0yB/2hqi6oqtVVtXr58uVzsUlJUjOZ/9H8CuD1SY4D9gB+BvgAsHeSpe1o4EBgS2u/BVgJbE6yFNgL+O5Q/YjhZSRJC8CERwpV9Y6qOrCqVjG4UHxdVf0ucD1wQmu2Driila9s07T511VVtfoT291JBwOHADfP2kgkSTM2mSOF8bwduCTJucBXgQtb/YXAp5JsAh5mECRU1Z1JLgM2Ak8Bp1fV0zPYviRplk0pFKrqBuCGVv4mY9w9VFU/At4wzvLnAedNtZOSpLnhJ5olSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpG7CUEiyR5Kbk3wtyZ1J/kurPzjJTUk2Jbk0yW6tfvc2vanNXzW0rne0+ruTHLujBiVJmp7JHCk8CRxdVb8EvBRYk+RI4N3A+6rq54FHgFNa+1OAR1r9+1o7khwKnAi8GFgDfDjJktkcjCRpZiYMhRp4vE3u2h4FHA1c3urXA8e38to2TZt/TJK0+kuq6smq+hawCTh8VkYhSZoVk7qmkGRJktuAh4CrgXuBR6vqqdZkM7CilVcA9wO0+Y8B+w7Xj7HM8LZOTbIhyYZt27ZNfUSSpGmbVChU1dNV9VLgQAbv7l+0ozpUVRdU1eqqWr18+fIdtRlJ0himdPdRVT0KXA+8HNg7ydI260BgSytvAVYCtPl7Ad8drh9jGUnSAjCZu4+WJ9m7lZ8F/BpwF4NwOKE1Wwdc0cpXtmna/Ouqqlr9ie3upIOBQ4CbZ2sgkqSZWzpxE/YH1rc7hXYBLquqq5JsBC5Jci7wVeDC1v5C4FNJNgEPM7jjiKq6M8llwEbgKeD0qnp6docjSZqJCUOhqm4HXjZG/TcZ4+6hqvoR8IZx1nUecN7UuylJmgt+olmS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6iYMhSQrk1yfZGOSO5Oc2eqfl+TqJPe0n/u0+iT5YJJNSW5PctjQuta19vckWbfjhiVJmo7JHCk8BfxhVR0KHAmcnuRQ4Czg2qo6BLi2TQO8BjikPU4FPgKDEAHOBo4ADgfOHgkSSdLCMGEoVNXWqrq1lb8P3AWsANYC61uz9cDxrbwW+GQN3AjsnWR/4Fjg6qp6uKoeAa4G1szqaCRJMzKlawpJVgEvA24C9quqrW3WA8B+rbwCuH9osc2tbrz60ds4NcmGJBu2bds2le5JkmZo0qGQ5LnAZ4C3VNX3hudVVQE1Gx2qqguqanVVrV6+fPlsrFKSNEmTCoUkuzIIhL+tqs+26gfbaSHaz4da/RZg5dDiB7a68eolSQvEZO4+CnAhcFdV/dXQrCuBkTuI1gFXDNWf1O5COhJ4rJ1m+iLw6iT7tAvMr251kqQFYukk2rwCeCNwR5LbWt0fA+cDlyU5Bfg28Ftt3ueB44BNwA+BNwFU1cNJ3gV8pbU7p6oenpVRSJJmxYShUFVfBjLO7GPGaF/A6eOs6yLgoql0UJI0d/xEsySpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUTRgKSS5K8lCSrw/VPS/J1UnuaT/3afVJ8sEkm5LcnuSwoWXWtfb3JFm3Y4YjSZqJyRwpfAJYM6ruLODaqjoEuLZNA7wGOKQ9TgU+AoMQAc4GjgAOB84eCRJJ0sIxYShU1ZeAh0dVrwXWt/J64Pih+k/WwI3A3kn2B44Frq6qh6vqEeBqnhk0kqR5Nt1rCvtV1dZWfgDYr5VXAPcPtdvc6sarlyQtIDO+0FxVBdQs9AWAJKcm2ZBkw7Zt22ZrtZKkSZhuKDzYTgvRfj7U6rcAK4faHdjqxqt/hqq6oKpWV9Xq5cuXT7N7kqTpmG4oXAmM3EG0DrhiqP6kdhfSkcBj7TTTF4FXJ9mnXWB+dauTJC0gSydqkORi4ChgWZLNDO4iOh+4LMkpwLeB32rNPw8cB2wCfgi8CaCqHk7yLuArrd05VTX64rUkaZ5NGApV9TvjzDpmjLYFnD7Oei4CLppS7yRJc8pPNEuSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeom/H8KkhaXVWd9rpfvO/+189gTLUYeKUiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ23pEqL3PAtqNJMeaQgSeoMBUlSZyhIkjpDQZLUeaF5gfF7ayTNJ48UJEmdoSBJ6gwFSVLnNYV55gePJC0khsICNjowvPAsaUczFDQj3i0l7Vy8piBJ6gwFSVLn6aN5sKMvLntKR5o9/9r+ngyFRcQ7lSTtaIaCpJ3Wv7Z3+bNhzkMhyRrgA8AS4ONVdf5c92E+LMR3+XP5B7Mjt+Wtu9LsmdNQSLIE+BDwa8Bm4CtJrqyqjXPZj53N9gJnsmE02y/aCzEEt2c64zeMtDOa6yOFw4FNVfVNgCSXAGuBnS4UFtuL4rDp9n02Ami2n7fprG82+jDddUw3kCbTbrLP9Y54U2BgLh6pqrnbWHICsKaqfr9NvxE4oqrOGGpzKnBqm3wJ8PU56+DcWwZ8Z747sQM5vsVtZx7fzjw2gF+oqj2ns+CCu9BcVRcAFwAk2VBVq+e5SzuM41vcHN/itTOPDQbjm+6yc/3htS3AyqHpA1udJGkBmOtQ+ApwSJKDk+wGnAhcOcd9kCSNY05PH1XVU0nOAL7I4JbUi6rqzu0scsHc9GzeOL7FzfEtXjvz2GAG45vTC82SpIXNL8STJHWGgiSpW1ChkOQNSe5M8pMk494ulmRNkruTbEpy1lz2cSaSPC/J1UnuaT/3Gafd00lua48FfyF+ov2RZPckl7b5NyVZNfe9nJ5JjO3kJNuG9tfvz0c/pyvJRUkeSjLm54Ey8ME2/tuTHDbXfZyuSYztqCSPDe27P5/rPs5EkpVJrk+ysb1unjlGm6nvv6paMA/gF4FfAG4AVo/TZglwL/BCYDfga8Ch8933SY7vPcBZrXwW8O5x2j0+332dwpgm3B/AfwI+2sonApfOd79ncWwnA389332dwRj/PXAY8PVx5h8HfAEIcCRw03z3eRbHdhRw1Xz3cwbj2x84rJX3BL4xxu/nlPffgjpSqKq7quruCZr1r8qoqv8HjHxVxmKwFljfyuuB4+exL7NlMvtjeNyXA8ckyRz2cboW8+/apFTVl4CHt9NkLfDJGrgR2DvJ/nPTu5mZxNgWtaraWlW3tvL3gbuAFaOaTXn/LahQmKQVwP1D05t55hOxUO1XVVtb+QFgv3Ha7ZFkQ5Ibkyz04JjM/uhtquop4DFg3znp3cxM9nftN9uh+eVJVo4xfzFbzH9vk/HyJF9L8oUkL57vzkxXOyX7MuCmUbOmvP/m46uzrwGeP8asP6mqK+a6P7Nte+MbnqiqSjLe/cAHVdWWJC8ErktyR1XdO9t91az4B+DiqnoyyR8wOCI6ep77pMm5lcHf2uNJjgP+Hjhknvs0ZUmeC3wGeEtVfW+m65vzUKiqV81wFQv6qzK2N74kDybZv6q2tkO4h8ZZx5b285tJbmDwDmChhsJk9sdIm81JlgJ7Ad+dm+7NyIRjq6rhcXycwXWjncmC/nubieEX0Kr6fJIPJ1lWVYvmi/KS7MogEP62qj47RpMp77/FePpoMX9VxpXAulZeBzzjyCjJPkl2b+VlwCtY2F8tPpn9MTzuE4Drql0FW+AmHNuo87OvZ3Bed2dyJXBSu4vlSOCxoVOgi1qS549c20pyOIPXw8XwZgUY3FkEXAjcVVV/NU6zqe+/+b6CPupK+a8zOOf1JPAg8MVWfwDw+VFX1L/B4N3zn8x3v6cwvn2Ba4F7gGuA57X61Qz+Cx3ArwB3MLjT5Q7glPnu9yTG9Yz9AZwDvL6V9wD+DtgE3Ay8cL77PItj+0vgzra/rgdeNN99nuL4Lga2Aj9uf3unAKcBp7X5YfCPse5tv49j3hW4EB+TGNsZQ/vuRuBX5rvPUxzfK4ECbgdua4/jZrr//JoLSVK3GE8fSZJ2EENBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnq/j/7k44e1aqKGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_by_average, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Combination by average\")\n",
    "plt.xlim(-1, 2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution now seems a bit a different and most values concentrate between -1 and 1. We then pick 1 as the threshold and create our clusters of outliers(1) and non outliers(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7519\n",
       "1     459\n",
       "Name: y_by_average_cluster, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(X_test)\n",
    "df_test['y_by_average_score'] = y_by_average\n",
    "df_test['y_by_average_cluster'] = np.where(df_test['y_by_average_score']<1, 0, 1)\n",
    "df_test['y_by_average_cluster'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flag_Pos_Neg_Mean</th>\n",
       "      <th>Flag_Ratio_of_Unique_Transactions_Mean</th>\n",
       "      <th>flag_amount_bin</th>\n",
       "      <th>flag_week_amount</th>\n",
       "      <th>flag_month_amount</th>\n",
       "      <th>y_by_average_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_by_average_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.096670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127289</td>\n",
       "      <td>0.026656</td>\n",
       "      <td>-0.142219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028655</td>\n",
       "      <td>0.286267</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>0.568043</td>\n",
       "      <td>0.692897</td>\n",
       "      <td>2.329727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Flag_Pos_Neg_Mean  \\\n",
       "y_by_average_cluster                      \n",
       "0                              0.000054   \n",
       "1                              0.028655   \n",
       "\n",
       "                      Flag_Ratio_of_Unique_Transactions_Mean  flag_amount_bin  \\\n",
       "y_by_average_cluster                                                            \n",
       "0                                                   0.096670         0.000000   \n",
       "1                                                   0.286267         0.014609   \n",
       "\n",
       "                      flag_week_amount  flag_month_amount  y_by_average_score  \n",
       "y_by_average_cluster                                                           \n",
       "0                             0.127289           0.026656           -0.142219  \n",
       "1                             0.568043           0.692897            2.329727  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby('y_by_average_cluster').mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean values of the outliers and non-outliers produces again a very big spread while also reducing the outliers to about 7% of the data. Lets try now with maximization and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7978.000000\n",
       "mean        0.000008\n",
       "std         1.000067\n",
       "min        -0.669417\n",
       "25%        -0.345234\n",
       "50%        -0.345234\n",
       "75%        -0.043724\n",
       "max        22.495321\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_by_maximization = maximization(test_scores_norm)\n",
    "pd.Series(y_by_maximization).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_by_maximization, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Combination by max\")\n",
    "plt.xlim(-1, 2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7519\n",
       "1     459\n",
       "Name: y_by_maximization_cluster, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['y_by_maximization_score'] = y_by_maximization\n",
    "df_test['y_by_maximization_cluster'] = np.where(df_test['y_by_maximization_score']<1, 0, 1)\n",
    "df_test['y_by_maximization_cluster'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flag_Pos_Neg_Mean</th>\n",
       "      <th>Flag_Ratio_of_Unique_Transactions_Mean</th>\n",
       "      <th>flag_amount_bin</th>\n",
       "      <th>flag_week_amount</th>\n",
       "      <th>flag_month_amount</th>\n",
       "      <th>y_by_average_score</th>\n",
       "      <th>y_by_average_cluster</th>\n",
       "      <th>y_by_maximization_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_by_maximization_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.096670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127289</td>\n",
       "      <td>0.026656</td>\n",
       "      <td>-0.142219</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.142211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028655</td>\n",
       "      <td>0.286267</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>0.568043</td>\n",
       "      <td>0.692897</td>\n",
       "      <td>2.329727</td>\n",
       "      <td>1</td>\n",
       "      <td>2.329740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Flag_Pos_Neg_Mean  \\\n",
       "y_by_maximization_cluster                      \n",
       "0                                   0.000054   \n",
       "1                                   0.028655   \n",
       "\n",
       "                           Flag_Ratio_of_Unique_Transactions_Mean  \\\n",
       "y_by_maximization_cluster                                           \n",
       "0                                                        0.096670   \n",
       "1                                                        0.286267   \n",
       "\n",
       "                           flag_amount_bin  flag_week_amount  \\\n",
       "y_by_maximization_cluster                                      \n",
       "0                                 0.000000          0.127289   \n",
       "1                                 0.014609          0.568043   \n",
       "\n",
       "                           flag_month_amount  y_by_average_score  \\\n",
       "y_by_maximization_cluster                                          \n",
       "0                                   0.026656           -0.142219   \n",
       "1                                   0.692897            2.329727   \n",
       "\n",
       "                           y_by_average_cluster  y_by_maximization_score  \n",
       "y_by_maximization_cluster                                                 \n",
       "0                                             0                -0.142211  \n",
       "1                                             1                 2.329740  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby('y_by_maximization_cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are almost identical. That means that the outliers detected by these three algorithms are fairly the same and thus our result does not seem to be suffering of overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing our Suspects\n",
    "\n",
    "Since the average and max cluster methods seem to produce similar results, lets look at instances where observations are marked outliers in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flag_Pos_Neg_Mean</th>\n",
       "      <th>Flag_Ratio_of_Unique_Transactions_Mean</th>\n",
       "      <th>flag_amount_bin</th>\n",
       "      <th>flag_week_amount</th>\n",
       "      <th>flag_month_amount</th>\n",
       "      <th>y_by_average_score</th>\n",
       "      <th>y_by_average_cluster</th>\n",
       "      <th>y_by_maximization_score</th>\n",
       "      <th>y_by_maximization_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34613</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.037696</td>\n",
       "      <td>1</td>\n",
       "      <td>2.037701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28858</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>1.390542</td>\n",
       "      <td>1</td>\n",
       "      <td>1.390554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38504</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>1.314098</td>\n",
       "      <td>1</td>\n",
       "      <td>1.314138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21661</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.059932</td>\n",
       "      <td>1</td>\n",
       "      <td>2.059941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25504</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.037696</td>\n",
       "      <td>1</td>\n",
       "      <td>2.037701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.595170</td>\n",
       "      <td>1</td>\n",
       "      <td>1.595173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32164</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.459651</td>\n",
       "      <td>1</td>\n",
       "      <td>1.459665</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19565</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.595170</td>\n",
       "      <td>1</td>\n",
       "      <td>1.595173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29453</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.595170</td>\n",
       "      <td>1</td>\n",
       "      <td>1.595173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32356</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.602249</td>\n",
       "      <td>1</td>\n",
       "      <td>1.602253</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30612</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.595170</td>\n",
       "      <td>1</td>\n",
       "      <td>1.595173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32824</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.037696</td>\n",
       "      <td>1</td>\n",
       "      <td>2.037701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.438016</td>\n",
       "      <td>1</td>\n",
       "      <td>2.438027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39875</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.733130</td>\n",
       "      <td>1</td>\n",
       "      <td>1.733140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6556</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.037696</td>\n",
       "      <td>1</td>\n",
       "      <td>2.037701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.595170</td>\n",
       "      <td>1</td>\n",
       "      <td>1.595173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22239</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.606048</td>\n",
       "      <td>1</td>\n",
       "      <td>1.606052</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16846</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.459651</td>\n",
       "      <td>1</td>\n",
       "      <td>1.459665</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5593</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.459651</td>\n",
       "      <td>1</td>\n",
       "      <td>1.459665</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15044</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.595170</td>\n",
       "      <td>1</td>\n",
       "      <td>1.595173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Flag_Pos_Neg_Mean  Flag_Ratio_of_Unique_Transactions_Mean  \\\n",
       "34613           0.000000                                0.000000   \n",
       "28858           0.000000                                0.653061   \n",
       "38504           0.105263                                0.000000   \n",
       "21661           0.000000                                1.000000   \n",
       "25504           0.000000                                0.000000   \n",
       "481             0.000000                                0.000000   \n",
       "32164           0.000000                                1.000000   \n",
       "19565           0.000000                                0.000000   \n",
       "29453           0.000000                                0.000000   \n",
       "32356           0.000000                                0.000000   \n",
       "30612           0.000000                                0.000000   \n",
       "32824           0.000000                                0.000000   \n",
       "4633            0.000000                                1.000000   \n",
       "39875           0.000000                                1.000000   \n",
       "6556            0.000000                                0.000000   \n",
       "13997           0.000000                                0.000000   \n",
       "22239           0.000000                                0.000000   \n",
       "16846           0.000000                                1.000000   \n",
       "5593            0.000000                                1.000000   \n",
       "15044           0.000000                                0.000000   \n",
       "\n",
       "       flag_amount_bin  flag_week_amount  flag_month_amount  \\\n",
       "34613              0.0          1.000000           1.000000   \n",
       "28858              0.0          0.673469           0.734694   \n",
       "38504              0.0          0.263158           0.052632   \n",
       "21661              0.0          0.000000           1.000000   \n",
       "25504              0.0          1.000000           1.000000   \n",
       "481                0.0          0.000000           1.000000   \n",
       "32164              0.0          1.000000           0.000000   \n",
       "19565              0.0          0.000000           1.000000   \n",
       "29453              0.0          0.000000           1.000000   \n",
       "32356              0.0          0.333333           1.000000   \n",
       "30612              0.0          0.000000           1.000000   \n",
       "32824              0.0          1.000000           1.000000   \n",
       "4633               0.0          1.000000           1.000000   \n",
       "39875              0.0          0.000000           0.833333   \n",
       "6556               0.0          1.000000           1.000000   \n",
       "13997              0.0          0.000000           1.000000   \n",
       "22239              0.0          0.347826           1.000000   \n",
       "16846              0.0          1.000000           0.000000   \n",
       "5593               0.0          1.000000           0.000000   \n",
       "15044              0.0          0.000000           1.000000   \n",
       "\n",
       "       y_by_average_score  y_by_average_cluster  y_by_maximization_score  \\\n",
       "34613            2.037696                     1                 2.037701   \n",
       "28858            1.390542                     1                 1.390554   \n",
       "38504            1.314098                     1                 1.314138   \n",
       "21661            2.059932                     1                 2.059941   \n",
       "25504            2.037696                     1                 2.037701   \n",
       "481              1.595170                     1                 1.595173   \n",
       "32164            1.459651                     1                 1.459665   \n",
       "19565            1.595170                     1                 1.595173   \n",
       "29453            1.595170                     1                 1.595173   \n",
       "32356            1.602249                     1                 1.602253   \n",
       "30612            1.595170                     1                 1.595173   \n",
       "32824            2.037696                     1                 2.037701   \n",
       "4633             2.438016                     1                 2.438027   \n",
       "39875            1.733130                     1                 1.733140   \n",
       "6556             2.037696                     1                 2.037701   \n",
       "13997            1.595170                     1                 1.595173   \n",
       "22239            1.606048                     1                 1.606052   \n",
       "16846            1.459651                     1                 1.459665   \n",
       "5593             1.459651                     1                 1.459665   \n",
       "15044            1.595170                     1                 1.595173   \n",
       "\n",
       "       y_by_maximization_cluster  \n",
       "34613                          1  \n",
       "28858                          1  \n",
       "38504                          1  \n",
       "21661                          1  \n",
       "25504                          1  \n",
       "481                            1  \n",
       "32164                          1  \n",
       "19565                          1  \n",
       "29453                          1  \n",
       "32356                          1  \n",
       "30612                          1  \n",
       "32824                          1  \n",
       "4633                           1  \n",
       "39875                          1  \n",
       "6556                           1  \n",
       "13997                          1  \n",
       "22239                          1  \n",
       "16846                          1  \n",
       "5593                           1  \n",
       "15044                          1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suspicious = df_test[(df_test['y_by_average_cluster'] == 1) & (df_test['y_by_maximization_cluster'] == 1 )]\n",
    "suspicious.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare outliers to non outliers to see if our results make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_outliers</th>\n",
       "      <th>mean_non_outliers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Flag_Pos_Neg_Mean</th>\n",
       "      <td>0.028655</td>\n",
       "      <td>0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flag_Ratio_of_Unique_Transactions_Mean</th>\n",
       "      <td>0.286267</td>\n",
       "      <td>0.096670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flag_amount_bin</th>\n",
       "      <td>0.014609</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flag_week_amount</th>\n",
       "      <td>0.568043</td>\n",
       "      <td>0.127289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flag_month_amount</th>\n",
       "      <td>0.692897</td>\n",
       "      <td>0.026656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_by_average_score</th>\n",
       "      <td>2.329727</td>\n",
       "      <td>-0.142219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_by_maximization_score</th>\n",
       "      <td>2.329740</td>\n",
       "      <td>-0.142211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        mean_outliers  mean_non_outliers\n",
       "Flag_Pos_Neg_Mean                            0.028655           0.000054\n",
       "Flag_Ratio_of_Unique_Transactions_Mean       0.286267           0.096670\n",
       "flag_amount_bin                              0.014609           0.000000\n",
       "flag_week_amount                             0.568043           0.127289\n",
       "flag_month_amount                            0.692897           0.026656\n",
       "y_by_average_score                           2.329727          -0.142219\n",
       "y_by_maximization_score                      2.329740          -0.142211"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_out = df_test[(df_test['y_by_average_cluster'] == 0) & (df_test['y_by_maximization_cluster'] == 0)]\n",
    "inspect_df = pd.DataFrame({'mean_outliers':suspicious.describe().loc['mean',:],\n",
    "              'mean_non_outliers':non_out.describe().loc['mean',:]})\n",
    "inspect_df.drop(['y_by_average_cluster', 'y_by_maximization_cluster'],inplace=True)\n",
    "inspect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "x = list(inspect_df.index)\n",
    "_X = np.arange(len(x))\n",
    "leg = ['Outliers','Non Outliers']\n",
    "\n",
    "plt.barh(_X - 0.2, inspect_df.loc[:,'mean_outliers'], 0.4)\n",
    "plt.barh(_X + 0.2, inspect_df.loc[:,'mean_non_outliers'], 0.4)\n",
    "plt.yticks(_X, x) # set labels manually\n",
    "plt.title('Stats of Suspects vs Entire Set',fontsize=20)\n",
    "plt.legend(leg,loc=(1.04,0.8))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The results show the huge gap between outliers and non-outliers. It is obvious then that the algorithm did its job right and was able to capture the instances of anomalies. In order to obtain a valid conclusion though, an in-depth investigation of each of these instances is needed. However, the results produced by the algorithm and the combination of methods could be used to create immediate red flags for credit card transactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
